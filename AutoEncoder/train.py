import torch
import pytorch_lightning as pl
from pytorch_lightning.callbacks import TQDMProgressBar, ModelCheckpoint
from callbacks.save_best_parts_reconstruction import SaveBestPartsAndReconstruction
from datamodules.mri_datamodule import MRIDataModule
from models.autoencoder import PreTrained_AutoEncoder
import os
import sys


def check_dependencies():
    """æ£€æŸ¥å¿…è¦çš„ä¾èµ–å’Œæ–‡ä»¶æ˜¯å¦å­˜åœ¨"""
    try:
        # æ£€æŸ¥æ•°æ®ç›®å½•
        root = "./data/MRI"
        if not os.path.exists(root):
            raise FileNotFoundError(f"MRI data directory not found: {root}")
        
        # æ£€æŸ¥æ•°æ®ç›®å½•æ˜¯å¦ä¸ºç©º
        if not os.listdir(root):
            raise FileNotFoundError(f"MRI data directory is empty: {root}")
        
        # æ£€æŸ¥å¿…è¦çš„æ¨¡å—æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        required_files = [
            "./datamodules/mri_datamodule.py",
            "./models/autoencoder.py",
            "./callbacks/save_best_parts_reconstruction.py"
        ]
        
        missing_files = []
        for file_path in required_files:
            if not os.path.exists(file_path):
                missing_files.append(file_path)
        
        if missing_files:
            print("Missing required files:")
            for file in missing_files:
                print(f"   - {file}")
            raise FileNotFoundError("Please ensure all required files exist")
        
        print("All dependencies and files check passed")
        return True
        
    except Exception as e:
        print(f"Dependency check failed: {e}")
        return False

def setup_device_info():
    
    if torch.cuda.is_available():
        device_count = torch.cuda.device_count()
        current_device = torch.cuda.current_device()
        device_name = torch.cuda.get_device_name(current_device)
        memory_total = torch.cuda.get_device_properties(current_device).total_memory / 1024**3
        
        print(f"   GPU Available: {device_name}")
        print(f"   Device count: {device_count}")
        print(f"   Memory: {memory_total:.1f} GB")
        print(f"   CUDA version: {torch.version.cuda}")
        
        # At least 4GB, otherwise cuda out of memory
        if memory_total < 4.0:
            print("Warning: GPU memory < 4GB, consider reducing batch_size")
        
        return "gpu", 1
    else:
        print("Using CPU (GPU not available)")
        return "cpu", 1
def create_callbacks():
    """åˆ›å»ºè®­ç»ƒå›è°ƒå‡½æ•°"""
    callbacks = [
        # è¿›åº¦æ¡
        TQDMProgressBar(refresh_rate=20),
        
        # æ¨¡å‹æ£€æŸ¥ç‚¹ä¿å­˜
        ModelCheckpoint(
            monitor="val_loss",
            filename="checkpoint-{epoch:02d}-{val_loss:.4f}",
            save_top_k=3,  # ä¿å­˜æœ€å¥½çš„3ä¸ªæ¨¡å‹
            mode="min",
            save_last=True,  # åŒæ—¶ä¿å­˜æœ€åä¸€ä¸ªepoch
            verbose=True
        ),
        
        # è‡ªå®šä¹‰çš„é‡å»ºå›¾åƒå’Œæœ€ä½³éƒ¨ä»¶ä¿å­˜
        SaveBestPartsAndReconstruction(
            save_dir="./checkpoints", 
            monitor="val_loss", 
            n_samples=8  # å¢åŠ æ ·æœ¬æ•°ç”¨äºæ›´å¥½çš„å¯è§†åŒ–
        )
    ]
    
    return callbacks
def create_optimized_datamodule():
    """åˆ›å»ºä¼˜åŒ–çš„æ•°æ®æ¨¡å—"""
    # æ ¹æ®å¯ç”¨å†…å­˜è°ƒæ•´å‚æ•°
    if torch.cuda.is_available():
        memory_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3
        if memory_gb >= 8:
            batch_size = 16
            num_workers = 8
        elif memory_gb >= 4:
            batch_size = 8
            num_workers = 4
        else:
            batch_size = 4
            num_workers = 2
    else:
        batch_size = 4
        num_workers = 2
    
    print(f"ğŸ“Š Using batch_size={batch_size}, num_workers={num_workers}")
    
    datamodule = MRIDataModule(
        dataset_name="mri_contrastive",
        root="./data/MRI",
        batch_size=batch_size,
        modalities=("T1", "T2"),
        num_workers=num_workers,
        fixed_pair=False,   
        allow_same_modality=True,
    )
    
    return datamodule
def create_optimized_model():
    """åˆ›å»ºä¼˜åŒ–çš„æ¨¡å‹"""
    # ä¸ºMRIå›¾åƒä¼˜åŒ–çš„å‚æ•°
    model = PreTrained_AutoEncoder(
        lr=1e-4,           
        lambda_rec=1.0,     
        lambda_nce=0.0,     
        proj_dim=128,       # 256
        temperature=0.1,    
    )
    
    return model

def main():

    print("Starting MRI AutoEncoder Training")
    print("=" * 50)
    
    os.makedirs("./checkpoints", exist_ok=True)
    work_dir = os.path.abspath("./checkpoints")
    
    print(f"Working directory: {work_dir}")

    # 1. check dependencies
    if not check_dependencies():
        print("Please fix the dependency issues before training")
        sys.exit(1)
    
    # 2. setup
    accelerator, devices = setup_device_info()
    
    # 3. datamodule
    try:
        datamodule = create_optimized_datamodule()
        print("DataModule created successfully")
    except Exception as e:
        print(f"Failed to create DataModule: {e}")
        sys.exit(1)
    
    # 4. model
    try:
        model = create_optimized_model()
        print("Model created successfully")
    except Exception as e:
        print(f"Failed to create model: {e}")
        sys.exit(1)
    
    # 5. callback
    callbacks = create_callbacks()
    
    # 6. trainer
    trainer = pl.Trainer(
        max_epochs=100,           # å¢åŠ è®­ç»ƒè½®æ•°
        accelerator=accelerator,
        devices=devices,
        logger=False,
        log_every_n_steps=10,
        callbacks=callbacks,
        default_root_dir=work_dir,
        
        gradient_clip_val=1.0,    # æ¢¯åº¦è£å‰ªï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
        deterministic=False,      # å…è®¸ä¸€äº›éšæœºæ€§ä»¥è·å¾—æ›´å¥½çš„æ€§èƒ½
        enable_progress_bar=True,
        enable_model_summary=True,
        
        # éªŒè¯è®¾ç½®
        val_check_interval=0.5,   # æ¯åŠä¸ªepochéªŒè¯ä¸€æ¬¡
        check_val_every_n_epoch=1,
        
    )
    

    try:
        print("\n Starting training...")
        print("=" * 50)
        trainer.fit(model, datamodule=datamodule)
        print("\n Training completed successfully!")
        
        if trainer.checkpoint_callback:
            best_path = trainer.checkpoint_callback.best_model_path
            best_score = trainer.checkpoint_callback.best_model_score
            print(f"Best model saved at: {best_path}")
            print(f"Best validation loss: {best_score:.4f}")
            
    except KeyboardInterrupt:
        print("\n  Training interrupted by user")
        print("   Partial results saved in checkpoints directory")
    except Exception as e:
        print(f"\n Training failed with error: {e}")
        print("   Please check your data and model configuration")
        sys.exit(1)

if __name__ == "__main__":
    main()
